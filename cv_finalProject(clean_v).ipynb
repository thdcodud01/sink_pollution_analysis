{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_35soE4uLIrI"
      },
      "source": [
        "ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/wkentaro/labelme.git"
      ],
      "metadata": {
        "id": "UuL0eIAQ9UVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_xMYhaQJ66x"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from skimage.feature import local_binary_pattern\n",
        "from ultralytics import YOLO\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import json\n",
        "import labelme\n",
        "import PIL.Image\n",
        "from labelme import utils\n",
        "\n",
        "# ğŸ“ êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸ ë° ê¸°ë³¸ ê²½ë¡œ ì„¤ì •\n",
        "drive.mount('/content/drive')\n",
        "base_dir = '/content/drive/MyDrive/SmartSan_Project/OpenImages_YOLO'\n",
        "os.makedirs(base_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4kpxv6XLGrD"
      },
      "source": [
        "í•„ìš”í•œ Open Images íŒŒì¼ ë‹¤ìš´ë¡œë“œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFDA5-o5KhUC"
      },
      "outputs": [],
      "source": [
        "# í´ë˜ìŠ¤ ì„¤ëª… íŒŒì¼\n",
        "!wget https://storage.googleapis.com/openimages/2018_04/class-descriptions-boxable.csv -P {base_dir}\n",
        "\n",
        "# validation bounding box\n",
        "!wget https://storage.googleapis.com/openimages/2018_04/validation/validation-annotations-bbox.csv -P {base_dir}\n",
        "\n",
        "# validation ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸\n",
        "!wget https://storage.googleapis.com/openimages/2018_04/validation/validation-images-with-rotation.csv -P {base_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-7WAPlpK9hf"
      },
      "source": [
        "- í•„ìš”í•œ ë¼ë²¨ë§Œ í•„í„°ë§í•´ì„œ ì €ì¥\n",
        "- í•´ë‹¹ ë¼ë²¨ì— í¬í•¨ëœ ì´ë¯¸ì§€ë“¤ë§Œ ë¦¬ìŠ¤íŠ¸ì—…"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgTzJPWGKTeo"
      },
      "outputs": [],
      "source": [
        "# ğŸ§¹ ëŒ€ìƒ í´ë˜ìŠ¤ í•„í„°ë§\n",
        "bbox_csv_path = f\"{base_dir}/validation-annotations-bbox.csv\"\n",
        "bbox_df = pd.read_csv(bbox_csv_path)\n",
        "target_classes = {\"/m/0c_jw\": \"sink\"}\n",
        "filtered_df = bbox_df[bbox_df['LabelName'].isin(target_classes.keys())]\n",
        "filtered_df.to_csv(f\"{base_dir}/filtered_validation_boxes.csv\", index=False)\n",
        "print(f\"âœ… ëŒ€ìƒ í´ë˜ìŠ¤ë§Œ ì¶”ì¶œ ì™„ë£Œ: {filtered_df.shape[0]}ê°œ bounding box\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBMtLUicK49l"
      },
      "source": [
        "ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ + ì €ì¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iGozVTvK3yV"
      },
      "outputs": [],
      "source": [
        "# ğŸ§³ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ\n",
        "image_ids = filtered_df['ImageID'].unique()[:500]  # ìµœëŒ€ 500ì¥\n",
        "image_dir = os.path.join(base_dir, 'images', 'train')\n",
        "os.makedirs(image_dir, exist_ok=True)\n",
        "\n",
        "base_img_url = \"https://open-images-dataset.s3.amazonaws.com/validation\"\n",
        "for img_id in tqdm(image_ids):\n",
        "    img_url = f\"{base_img_url}/{img_id}.jpg\"\n",
        "    img_path = os.path.join(image_dir, f\"{img_id}.jpg\")\n",
        "    try:\n",
        "        r = requests.get(img_url, timeout=10)\n",
        "        with open(img_path, 'wb') as f:\n",
        "            f.write(r.content)\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ {img_id} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP4hlYzWLvAv"
      },
      "source": [
        "YOLO í¬ë§· ë¼ë²¨ íŒŒì¼ ìƒì„±\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZUDwrrcLt1g"
      },
      "outputs": [],
      "source": [
        "# ğŸ·ï¸ YOLO í¬ë§· ë¼ë²¨ ìƒì„±\n",
        "label_dir = os.path.join(base_dir, 'labels', 'train')\n",
        "os.makedirs(label_dir, exist_ok=True)\n",
        "class_id_map = {\"/m/0c_jw\": 0}  # sink í´ë˜ìŠ¤ ë¼ë²¨\n",
        "grouped = filtered_df.groupby('ImageID')\n",
        "for img_id, group in grouped:\n",
        "    if img_id not in image_ids:\n",
        "        continue\n",
        "    label_path = os.path.join(label_dir, f\"{img_id}.txt\")\n",
        "    with open(label_path, 'w') as f:\n",
        "        for _, row in group.iterrows():\n",
        "            class_id = class_id_map[row['LabelName']]\n",
        "            x_center = (row['XMin'] + row['XMax']) / 2\n",
        "            y_center = (row['YMin'] + row['YMax']) / 2\n",
        "            width = row['XMax'] - row['XMin']\n",
        "            height = row['YMax'] - row['YMin']\n",
        "            f.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j2Gf8hDMQlb"
      },
      "source": [
        "data.yaml ìƒì„± (YOLO í•™ìŠµ ì„¤ì •)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hA4cf6evMPzd"
      },
      "outputs": [],
      "source": [
        "# ğŸ“„ data.yaml ìƒì„±\n",
        "yaml_path = os.path.join(base_dir, \"data.yaml\")\n",
        "with open(yaml_path, 'w') as f:\n",
        "    f.write(f\"\"\"\n",
        "path: {base_dir}\n",
        "train: images/train\n",
        "val: images/train\n",
        "\n",
        "names:\n",
        "  0: sink\n",
        "\"\"\")\n",
        "print(\"âœ… data.yaml ìƒì„± ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrti1xlfMT5J"
      },
      "source": [
        "YOLOv8 í•™ìŠµ ì‹œì‘"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V730ceZTmdJg"
      },
      "outputs": [],
      "source": [
        "# ğŸ§³ ì‹¤ì‚¬ ì´ë¯¸ì§€ ë° ë¼ë²¨ ìë™ ë³µì‚¬\n",
        "zip_extracted_path = '/content/drive/MyDrive/sink_labels/'  # ë¼ë²¨ë§ zip í‘¼ ê²½ë¡œ\n",
        "img_train_path = os.path.join(base_dir, 'images/train')\n",
        "label_train_path = os.path.join(base_dir, 'labels/train')\n",
        "\n",
        "for file in os.listdir(zip_extracted_path):\n",
        "    if file.endswith('.jpg'):\n",
        "        shutil.copy(os.path.join(zip_extracted_path, file), img_train_path)\n",
        "    elif file.endswith('.txt'):\n",
        "        shutil.copy(os.path.join(zip_extracted_path, file), label_train_path)\n",
        "\n",
        "print(\"âœ… ì‹¤ì‚¬ ì´ë¯¸ì§€ ë° ë¼ë²¨ ì •ë¦¬ ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5IHxEr4MVaA"
      },
      "outputs": [],
      "source": [
        "# ğŸ§³ ì‹¤ì‚¬ ì´ë¯¸ì§€ ë° ë¼ë²¨ ìë™ ë³µì‚¬\n",
        "zip_extracted_path = '/content/drive/MyDrive/sink_labels/'  # ë¼ë²¨ë§ zip í‘¼ ê²½ë¡œ\n",
        "img_train_path = os.path.join(base_dir, 'images/train')\n",
        "label_train_path = os.path.join(base_dir, 'labels/train')\n",
        "\n",
        "for file in os.listdir(zip_extracted_path):\n",
        "    if file.endswith('.jpg'):\n",
        "        shutil.copy(os.path.join(zip_extracted_path, file), img_train_path)\n",
        "    elif file.endswith('.txt'):\n",
        "        shutil.copy(os.path.join(zip_extracted_path, file), label_train_path)\n",
        "print(\"âœ… ì‹¤ì‚¬ ì´ë¯¸ì§€ ë° ë¼ë²¨ ì •ë¦¬ ì™„ë£Œ!\")\n",
        "\n",
        "# ğŸ” Fine-tuning ì‹œì‘\n",
        "model = YOLO('yolov8n.pt')  # ê¸°ì¡´ best.pt ëŒ€ì‹  ê²½ëŸ‰ YOLO ëª¨ë¸ë¡œ ì‹œì‘\n",
        "model.train(\n",
        "    data=yaml_path,\n",
        "    epochs=25,\n",
        "    imgsz=960,\n",
        "    batch=8\n",
        ")\n",
        "print(\"âœ… Fine-tuning ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spLthue7blko"
      },
      "source": [
        "íƒì§€ ëª¨ë¸ ë¡œë”© ë° ì ìš©(í…ŒìŠ¤íŠ¸ ë°ì´í„°)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDJb9updFWcr"
      },
      "outputs": [],
      "source": [
        "# crop ë° output í´ë” ì´ˆê¸°í™”\n",
        "crop_dir = '/content/drive/MyDrive/SmartSan_Project/cropped_dataset/sink'\n",
        "output_dir = '/content/drive/MyDrive/SmartSan_Project/output'\n",
        "\n",
        "for path in [crop_dir, output_dir]:\n",
        "    if os.path.exists(path):\n",
        "        shutil.rmtree(path)\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "print(\"âœ… crop_dir ë° output_dir ì´ˆê¸°í™” ì™„ë£Œ\")\n",
        "\n",
        "# YOLO ëª¨ë¸ ë¡œë“œ\n",
        "model = YOLO('runs/detect/train/weights/best.pt')\n",
        "\n",
        "# ê²½ë¡œ ì„¤ì •\n",
        "sink_labels_dirs = [\n",
        "    '/content/drive/MyDrive/sink_labels',  # ì²« ë²ˆì§¸ ë¼ë²¨ í´ë”\n",
        "    '/content/drive/MyDrive/sink_labels/sink_labels2',  # ë‘ ë²ˆì§¸ ë¼ë²¨ í´ë”\n",
        "    '/content/drive/MyDrive/sink_labels/sink_labels3'  # ì„¸ ë²ˆì§¸ ë¼ë²¨ í´ë”\n",
        "]\n",
        "raw_images_dir = '/content/drive/MyDrive/SmartSan_Project/raw_images'\n",
        "output_dir = '/content/drive/MyDrive/SmartSan_Project/output'\n",
        "os.makedirs(raw_images_dir, exist_ok=True)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# ì´ë¯¸ì§€ íŒŒì¼ ìë™ ë³µì‚¬ (sink_labels2 ë¼ë²¨ íŒŒì¼ì— ë§ëŠ” ì´ë¯¸ì§€ ë³µì‚¬)\n",
        "for directory in sink_labels_dirs:\n",
        "    label_files = [f for f in os.listdir(directory) if f.endswith('.txt')]  # .txt ë¼ë²¨ íŒŒì¼ë§Œ ì½ê¸°\n",
        "\n",
        "    for label_file in label_files:\n",
        "        # ì´ë¯¸ì§€ íŒŒì¼ëª… ìƒì„±\n",
        "        img_name = label_file.replace('.txt', '.jpg')\n",
        "        img_path = os.path.join(raw_images_dir, img_name)\n",
        "\n",
        "        # ì´ë¯¸ì§€ê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸\n",
        "        if not os.path.exists(img_path):\n",
        "            # ì´ë¯¸ì§€ëŠ” raw_images í´ë”ì— ë„£ê¸° ìœ„í•´ì„œ ë³µì‚¬\n",
        "            src_img_path = os.path.join(directory, img_name)\n",
        "\n",
        "            if os.path.exists(src_img_path):  # ì›ë³¸ ì´ë¯¸ì§€ê°€ ìˆëŠ”ì§€ í™•ì¸\n",
        "                shutil.copy(src_img_path, img_path)  # ì´ë¯¸ì§€ë¥¼ raw_images í´ë”ë¡œ ë³µì‚¬\n",
        "                print(f\"âœ… {img_name} ì´ë¯¸ì§€ ë³µì‚¬ ì™„ë£Œ.\")\n",
        "            else:\n",
        "                print(f\"âŒ {img_name} ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        else:\n",
        "            print(f\"âœ… {img_name} ì´ë¯¸ì§€ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "# ìë™ìœ¼ë¡œ íƒì§€ ë° ê²°ê³¼ ì €ì¥\n",
        "image_files = [f for f in os.listdir(raw_images_dir) if f.endswith('.jpg')]\n",
        "\n",
        "# ì´ë¯¸ì§€ê°€ ìˆëŠ”ì§€ í™•ì¸\n",
        "if len(image_files) == 0:\n",
        "    print(\"âŒ raw_images í´ë”ì— ì´ë¯¸ì§€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "else:\n",
        "    print(f\"âœ… ì´ {len(image_files)}ê°œì˜ ì´ë¯¸ì§€ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "# YOLO íƒì§€ ìˆ˜í–‰ ë° ê²°ê³¼ ì €ì¥\n",
        "for img_file in image_files:\n",
        "    img_path = os.path.join(raw_images_dir, img_file)\n",
        "\n",
        "    if not os.path.exists(img_path):\n",
        "        print(f\"âŒ {img_file} ì—†ìŒ - ìŠ¤í‚µ\")\n",
        "        continue\n",
        "\n",
        "    # YOLO íƒì§€ ìˆ˜í–‰\n",
        "    results = model(img_path)\n",
        "\n",
        "    # ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥\n",
        "    save_path = os.path.join(output_dir, f\"result_{img_file}\")\n",
        "    results[0].save(filename=save_path)\n",
        "\n",
        "    print(f\"âœ… íƒì§€ ì™„ë£Œ: {img_file} ì €ì¥ë¨ -> {save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAnCiPgKscAq"
      },
      "source": [
        "mAP í•™ìŠµ ê²°ê³¼ ì‹œê°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsOTu9fsre_1"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "# mAP ê·¸ë˜í”„ ì‹œê°í™”\n",
        "result_graph_path = 'runs/detect/train/results.png'\n",
        "display(Image(filename=result_graph_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXh0E9RCs4RP"
      },
      "source": [
        "YOLO ê¸°ë°˜ ìë™ crop ì½”ë“œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b60ulwSesgeg"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "# YOLO ëª¨ë¸ ë¡œë“œ\n",
        "model = YOLO('runs/detect/train/weights/best.pt')\n",
        "\n",
        "# í´ë” ì„¤ì •\n",
        "raw_dir = '/content/drive/MyDrive/SmartSan_Project/raw_images'\n",
        "crop_dir = '/content/drive/MyDrive/SmartSan_Project/cropped_dataset/sink'\n",
        "os.makedirs(crop_dir, exist_ok=True)\n",
        "\n",
        "# raw_images í´ë”ì— ìˆëŠ” ëª¨ë“  .jpg ì´ë¯¸ì§€ íŒŒì¼ ì½ê¸°\n",
        "image_files = [f for f in os.listdir(raw_dir) if f.endswith('.jpg')]\n",
        "\n",
        "# ì´ë¯¸ì§€ê°€ ìˆëŠ”ì§€ í™•ì¸\n",
        "if len(image_files) == 0:\n",
        "    print(\"âŒ raw_images í´ë”ì— ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "else:\n",
        "    print(f\"âœ… ì´ {len(image_files)}ê°œì˜ ì´ë¯¸ì§€ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "# ì´ë¯¸ì§€ ë°˜ë³µ íƒì§€ ë° í¬ë¡­\n",
        "for img_file in image_files:\n",
        "    img_path = os.path.join(raw_dir, img_file)\n",
        "\n",
        "    if not os.path.exists(img_path):\n",
        "        print(f\"âŒ {img_file} ì—†ìŒ - ìŠ¤í‚µ\")\n",
        "        continue\n",
        "\n",
        "    # YOLO íƒì§€\n",
        "    results = model(img_path)\n",
        "    img = Image.open(img_path)\n",
        "\n",
        "    # sink class = 0 (sinkë§Œ í¬ë¡­)\n",
        "    for j, box in enumerate(results[0].boxes):\n",
        "        cls_id = int(box.cls[0])\n",
        "        if cls_id == 0:  # sinkë§Œ í¬ë¡­\n",
        "            xyxy = box.xyxy[0].cpu().numpy()\n",
        "            cropped = img.crop(xyxy)\n",
        "\n",
        "            # ì´ë¯¸ì§€ë¥¼ RGBë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥ (RGBA -> RGB)\n",
        "            cropped = cropped.convert(\"RGB\")\n",
        "\n",
        "            crop_path = os.path.join(crop_dir, f'{img_file[:-4]}_crop{j}.jpg')\n",
        "            cropped.save(crop_path)\n",
        "            print(f\"âœ… ì €ì¥ë¨: {crop_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "U-Net ëª¨ë¸ ì •ì˜"
      ],
      "metadata": {
        "id": "VNgl0wDLpVN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=1):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        def conv_block(in_ch, out_ch):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "\n",
        "        self.enc1 = conv_block(in_channels, 64)\n",
        "        self.enc2 = conv_block(64, 128)\n",
        "        self.enc3 = conv_block(128, 256)\n",
        "        self.enc4 = conv_block(256, 512)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.bottleneck = conv_block(512, 1024)\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
        "        self.dec4 = conv_block(1024, 512)\n",
        "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.dec3 = conv_block(512, 256)\n",
        "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.dec2 = conv_block(256, 128)\n",
        "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.dec1 = conv_block(128, 64)\n",
        "\n",
        "        self.out = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc1 = self.enc1(x)\n",
        "        enc2 = self.enc2(self.pool(enc1))\n",
        "        enc3 = self.enc3(self.pool(enc2))\n",
        "        enc4 = self.enc4(self.pool(enc3))\n",
        "        bottleneck = self.bottleneck(self.pool(enc4))\n",
        "\n",
        "        dec4 = self.dec4(torch.cat((self.upconv4(bottleneck), enc4), dim=1))\n",
        "        dec3 = self.dec3(torch.cat((self.upconv3(dec4), enc3), dim=1))\n",
        "        dec2 = self.dec2(torch.cat((self.upconv2(dec3), enc2), dim=1))\n",
        "        dec1 = self.dec1(torch.cat((self.upconv1(dec2), enc1), dim=1))\n",
        "\n",
        "        return torch.sigmoid(self.out(dec1))"
      ],
      "metadata": {
        "id": "pQTma2vHpLfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dice Loss + BCEDiceLoss"
      ],
      "metadata": {
        "id": "cxIyq7K95OoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DiceLossì—ì„œëŠ” sigmoid ì œê±° (ì…ë ¥ì— ì´ë¯¸ sigmoid ë˜ì–´ ìˆìœ¼ë‹ˆê¹Œ)\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, smooth=1.0):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "\n",
        "        intersection = (inputs * targets).sum()\n",
        "        dice_score = (2. * intersection + self.smooth) / (inputs.sum() + targets.sum() + self.smooth)\n",
        "        return 1 - dice_score\n",
        "\n",
        "\n",
        "class BCEDiceLoss(nn.Module):\n",
        "    def __init__(self, dice_weight=1.5):\n",
        "        super(BCEDiceLoss, self).__init__()\n",
        "        self.bce = nn.BCELoss()\n",
        "        self.dice = DiceLoss()\n",
        "        self.dice_weight = dice_weight\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        bce_loss = self.bce(inputs, targets)\n",
        "        dice_loss = self.dice(inputs, targets)\n",
        "        return bce_loss + self.dice_weight * dice_loss"
      ],
      "metadata": {
        "id": "L5CkYdsc5QHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "json â†’ mask ë³€í™˜ ì‹œì‘"
      ],
      "metadata": {
        "id": "aGxN-yvKzH9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install albumentations"
      ],
      "metadata": {
        "id": "-uoP5eBRf8Fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms.functional as TF\n",
        "import random\n",
        "\n",
        "class JointTransform:\n",
        "    def __call__(self, image, mask):\n",
        "        if random.random() > 0.5:\n",
        "            image = TF.hflip(image)\n",
        "            mask = TF.hflip(mask)\n",
        "        if random.random() > 0.5:\n",
        "            angle = random.uniform(-15, 15)\n",
        "            image = TF.rotate(image, angle)\n",
        "            mask = TF.rotate(mask, angle)\n",
        "        return image, mask\n",
        "\n",
        "\n",
        "resize = transforms.Resize((256, 256))\n",
        "\n",
        "class SinkSegmentationDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None, augment=False):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.augment = augment\n",
        "        self.joint_transform = JointTransform()\n",
        "        self.image_filenames = sorted([f for f in os.listdir(image_dir) if f.endswith(('jpg', 'png', 'jpeg'))])\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_filenames[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "\n",
        "        # ë§ˆìŠ¤í¬ íŒŒì¼ ì´ë¦„ ìƒì„± (ì¤‘ë³µ ë°©ì§€)\n",
        "        mask_filename = os.path.splitext(img_name)[0] + '_mask.png'\n",
        "        mask_path = os.path.join(self.mask_dir, mask_filename)\n",
        "\n",
        "        # ì´ë¯¸ì§€/ë§ˆìŠ¤í¬ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "        mask = np.array(Image.open(mask_path).convert(\"L\"))\n",
        "\n",
        "        # ì¦ê°• ì ìš© (í•™ìŠµìš©ì—ë§Œ)\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image, mask=mask)\n",
        "            image = augmented['image']\n",
        "            mask = augmented['mask'].unsqueeze(0).float() / 255.0 # 1ì±„ë„ë¡œ ë§Œë“¤ê³  float ë³€í™˜\n",
        "        else:\n",
        "            image = transforms.ToTensor()(image)\n",
        "            mask = transforms.ToTensor()(mask)\n",
        "\n",
        "        return image, mask"
      ],
      "metadata": {
        "id": "mk3h8DkHy1f7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë°ì´í„°ì…‹ ë¡œë”©"
      ],
      "metadata": {
        "id": "T9doZosWGlXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ“Š í•™ìŠµ ë°ì´í„° ì¤€ë¹„\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "# Albumentations transform ì •ì˜\n",
        "train_transform = A.Compose([\n",
        "    A.Resize(256, 256),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.Rotate(limit=15, p=0.5),\n",
        "    A.ColorJitter(brightness=0.2, contrast=0.2, p=0.5),\n",
        "    A.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=0.3),\n",
        "    A.Normalize(),\n",
        "    ToTensorV2()\n",
        "])\n",
        "val_transform = A.Compose([\n",
        "    A.Resize(256, 256),\n",
        "    A.Normalize(),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "# ê²½ë¡œ ì„¤ì •\n",
        "image_dir = \"/content/drive/MyDrive/SmartSan_Project/segmentation_labelme/images\"\n",
        "mask_dir = \"/content/drive/MyDrive/SmartSan_Project/segmentation_labelme/masks\"\n",
        "\n",
        "# ì¸ë±ìŠ¤ ë¶„í• \n",
        "all_indices = list(range(len(os.listdir(image_dir))))\n",
        "train_indices, val_indices = train_test_split(all_indices, test_size=0.2, random_state=42)\n",
        "\n",
        "# Dataset ì •ì˜ ë° Subset ë‚˜ëˆ„ê¸°\n",
        "train_dataset_full = SinkSegmentationDataset(image_dir, mask_dir, transform=train_transform)\n",
        "val_dataset_full = SinkSegmentationDataset(image_dir, mask_dir, transform=val_transform)\n",
        "\n",
        "train_dataset = Subset(train_dataset_full, train_indices)\n",
        "val_dataset = Subset(val_dataset_full, val_indices)\n",
        "\n",
        "# DataLoader ì„¤ì •\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# í•™ìŠµ ì„¤ì •\n",
        "import torch.optim as optim\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = UNet().to(device)\n",
        "criterion = BCEDiceLoss(dice_weight=1.5)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# ë””ë²„ê¹…\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "\n",
        "# í•™ìŠµ ë£¨í”„\n",
        "from tqdm import tqdm\n",
        "num_epochs = 30\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device).squeeze(1)\n",
        "        outputs = model(images).squeeze(1)\n",
        "        loss = criterion(outputs, masks)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    avg_loss = train_loss / len(train_loader)\n",
        "    print(f\"âœ… Epoch {epoch+1} - í‰ê·  í•™ìŠµ ì†ì‹¤: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "hitYP6SdHdvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ìºì‹œ ë° ë¶ˆí•„ìš”í•œ ê°ì²´ ì •ë¦¬"
      ],
      "metadata": {
        "id": "TiUMrzrd2FyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "_DZ1uL8j2DNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ëª¨ë¸ ì‹œê°í™” ì½”ë“œ"
      ],
      "metadata": {
        "id": "V5JaXM7S2J3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •\n",
        "model.eval()\n",
        "\n",
        "# íŒŒì¼ ê²½ë¡œ\n",
        "img_path = '/content/drive/MyDrive/SmartSan_Project/segmentation_labelme/images/test41.jpg'\n",
        "mask_path = '/content/drive/MyDrive/SmartSan_Project/segmentation_labelme/masks/test41_mask.png'\n",
        "\n",
        "# ì „ì²˜ë¦¬\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "img = Image.open(img_path).convert(\"RGB\")\n",
        "input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "# ë§ˆìŠ¤í¬ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "mask = Image.open(mask_path).convert(\"L\").resize((256, 256))\n",
        "mask_tensor = transforms.ToTensor()(mask).squeeze().numpy()\n",
        "\n",
        "# ì¶”ë¡ \n",
        "with torch.no_grad():\n",
        "    pred = model(input_tensor)\n",
        "    pred_mask = (pred.squeeze().cpu().numpy() > 0.5).astype(int)\n",
        "\n",
        "# ì‹œê°í™”\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(img)\n",
        "plt.title(\"Input Image\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(mask_tensor, cmap=\"gray\")\n",
        "plt.title(\"Ground Truth\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(pred_mask, cmap=\"gray\")\n",
        "plt.title(\"Predicted Mask\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EAL67M-G2Dq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì „ì²´ ì˜ˆì¸¡ ì‹œê°í™” + ì €ì¥"
      ],
      "metadata": {
        "id": "B1xaWIPJgiM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_with_status(model, dataset, index, device, save_dir, show=True):\n",
        "    model.eval()\n",
        "    image, mask = dataset[index]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_input = image.unsqueeze(0).to(device)\n",
        "        output = model(image_input).squeeze().cpu()\n",
        "\n",
        "    pred_mask = (output > 0.1).float()\n",
        "    contamination = pred_mask.sum().item() / pred_mask.numel() * 100\n",
        "\n",
        "    # ğŸ¯ 3ë‹¨ê³„ ì˜¤ì—¼ë„ ê¸°ì¤€ (0~15 clean, 15~30 moderate, 30~ dirty)\n",
        "    if contamination < 15:\n",
        "        status = \"Clean\"\n",
        "        box_color = 'blue'\n",
        "    elif contamination < 30:\n",
        "        status = \"Moderate\"\n",
        "        box_color = 'orange'\n",
        "    else:\n",
        "        status = \"Dirty\"\n",
        "        box_color = 'red'\n",
        "\n",
        "    # ì‹œê°í™”\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
        "\n",
        "    axs[0].imshow(image.permute(1, 2, 0).numpy())\n",
        "    axs[0].set_title(f\"Input Image\\n[{status}] {contamination:.2f}%\")\n",
        "    axs[0].add_patch(plt.Rectangle(\n",
        "        (0, 0), image.shape[2], image.shape[1],\n",
        "        linewidth=4, edgecolor=box_color, facecolor='none'\n",
        "    ))\n",
        "\n",
        "    axs[1].imshow(mask.squeeze().numpy(), cmap='gray')\n",
        "    axs[1].set_title(\"Ground Truth\")\n",
        "\n",
        "    axs[2].imshow(pred_mask.squeeze().numpy(), cmap='gray')\n",
        "    axs[2].set_title(\"Predicted Mask\")\n",
        "\n",
        "    for ax in axs:\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    save_path = os.path.join(save_dir, f\"result_{index}.png\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()\n",
        "\n",
        "save_dir = \"/content/drive/MyDrive/SmartSan_Project/results\"\n",
        "full_dataset = SinkSegmentationDataset(image_dir, mask_dir, transform=val_transform)\n",
        "\n",
        "print(f\"ğŸ“Š ì „ì²´ ë°ì´í„°ì…‹ ì´ë¯¸ì§€ ìˆ˜: {len(full_dataset)}\")\n",
        "\n",
        "# ì „ì²´ ì´ë¯¸ì§€ ì‹œê°í™” + ì €ì¥\n",
        "for i in range(len(full_dataset)):\n",
        "    print(f\"ğŸ”„ ì´ë¯¸ì§€ {i} ì²˜ë¦¬ ì¤‘...\")\n",
        "    visualize_with_status(model, full_dataset, index=i, device=device, save_dir=save_dir, show=True)"
      ],
      "metadata": {
        "id": "9Kuofxo2gjxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion Matrix + Classification Report"
      ],
      "metadata": {
        "id": "WSToDYMwuLfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "for i in range(len(full_dataset)):\n",
        "    image, mask = full_dataset[i]\n",
        "    image_input = image.unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(image_input).squeeze().cpu()\n",
        "\n",
        "    # ì˜ˆì¸¡ ì˜¤ì—¼ë„ (%)\n",
        "    pred_mask = (output > 0.1).float()\n",
        "    contamination = 100.0 * pred_mask.sum().item() / pred_mask.numel()\n",
        "\n",
        "    # ì˜ˆì¸¡ í´ë˜ìŠ¤\n",
        "    if contamination <= 15:\n",
        "        pred = 0  # Clean\n",
        "    elif contamination <= 30:\n",
        "        pred = 1  # Moderate\n",
        "    else:\n",
        "        pred = 2  # Dirty\n",
        "    pred_labels.append(pred)\n",
        "\n",
        "    # ì‹¤ì œ ì˜¤ì—¼ë„ (%)\n",
        "    true_contam = 100.0 * mask.sum().item() / mask.numel()\n",
        "    if true_contam <= 15:\n",
        "        true = 0\n",
        "    elif true_contam <= 30:\n",
        "        true = 1\n",
        "    else:\n",
        "        true = 2\n",
        "    true_labels.append(true)\n",
        "\n",
        "# ğŸ”¢ Confusion Matrix\n",
        "cm = confusion_matrix(true_labels, pred_labels)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Clean\", \"Moderate\", \"Dirty\"])\n",
        "disp.plot(cmap=\"Blues\")\n",
        "plt.title(\"ğŸ“‰ Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# ğŸ“„ Classification Report\n",
        "report = classification_report(true_labels, pred_labels, target_names=[\"Clean\", \"Moderate\", \"Dirty\"])\n",
        "print(\"ğŸ“‹ Classification Report:\\n\")\n",
        "print(report)"
      ],
      "metadata": {
        "id": "Uw_jeKZ6uMMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!zip -r /content/drive/MyDrive/SmartSan_Project.zip /content/drive/MyDrive/SmartSan_Project"
      ],
      "metadata": {
        "id": "uRmCALnYVzE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DkJujQ9yVzrZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}